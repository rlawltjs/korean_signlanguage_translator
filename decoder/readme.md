# í•œêµ­ ìˆ˜ì–´ ì¸ì‹ ë° ë²ˆì—­ ì‹œìŠ¤í…œ (ìˆ˜ì •ì¤‘)

OpenHands ëª¨ë¸ ê¸°ë°˜ì˜ í•œêµ­ì–´ ìˆ˜ì–´ ë¹„ë””ì˜¤ë¥¼ ìì—°ì–´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” AI ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸš€ ì£¼ìš” ê¸°ëŠ¥ (í…ŒìŠ¤íŠ¸ì¤‘)

- **ë‹¨ê³„ì  í•™ìŠµ**: ë‹¨ì–´ ë‹¨ìœ„ â†’ ë¬¸ì¥ ë‹¨ìœ„ ì ì§„ì  í•™ìŠµ
- **Encoder-Decoder ì•„í‚¤í…ì²˜**: OpenHands (ìˆ˜ì–´) + í•œêµ­ì–´ LLM (í…ìŠ¤íŠ¸)
- **ì‹¤ì‹œê°„ ë²ˆì—­**: ì›¹ìº  ë˜ëŠ” ë¹„ë””ì˜¤ íŒŒì¼ì—ì„œ ì‹¤ì‹œê°„ ìˆ˜ì–´ ë²ˆì—­
- **ë¬¸ë§¥ ì¸ì‹**: ì´ì „ ëŒ€í™”ë¥¼ ê³ ë ¤í•œ ìì—°ìŠ¤ëŸ¬ìš´ ë²ˆì—­
- **ë‹¤ì–‘í•œ ë””ì½”ë”©**: ë¹” ì„œì¹˜, ë¬¸ë²• ì œì•½, ê°ì • ì¸ì‹

## ğŸ“‹ ìš”êµ¬ì‚¬í•­

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­
- Python 3.8+
- CUDA 11.0+ (GPU ì‚¬ìš©ì‹œ) ë˜ëŠ” Intel GPU
- RAM 8GB+ ê¶Œì¥
- ì €ì¥ê³µê°„ 10GB+ (ëª¨ë¸ ë° ë°ì´í„°)

### í•„ìˆ˜ íŒ¨í‚¤ì§€
```bash
pip install torch torchvision torchaudio
pip install transformers
pip install mediapipe
pip install opencv-python
pip install numpy pandas
pip install tqdm
pip install pathlib
```

### ì„ íƒ íŒ¨í‚¤ì§€ (ê³ ê¸‰ ê¸°ëŠ¥)
```bash
# Intel GPU ì§€ì›
pip install intel_extension_for_pytorch

# í•œêµ­ì–´ NLP
pip install konlpy

# í‰ê°€ ë©”íŠ¸ë¦­
pip install nltk sacrebleu
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
korean_sign_recognition/
â”œâ”€â”€ data_preprocessor.py          # ë°ì´í„° ì „ì²˜ë¦¬
â”œâ”€â”€ openhands_finetuner.py       # OpenHands ëª¨ë¸ íŒŒì¸íŠœë‹
â”œâ”€â”€ encoder_decoder_model.py     # Encoder-Decoder ëª¨ë¸
â”œâ”€â”€ advanced_sign_to_text.py     # ê³ ê¸‰ ê¸°ëŠ¥ë“¤
â”œâ”€â”€ main.py                      # ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ aihub_sign_data/         # AIHub ì›ë³¸ ë°ì´í„°
â”‚   â”œâ”€â”€ processed_data/          # ì „ì²˜ë¦¬ëœ ë°ì´í„°
â”‚   â””â”€â”€ text_annotations.json   # ìì—°ì–´ ë¬¸ì¥ ì–´ë…¸í…Œì´ì…˜
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ openhands_pretrained/    # ì‚¬ì „í›ˆë ¨ëœ OpenHands ëª¨ë¸
â”‚   â”œâ”€â”€ word_level/              # ë‹¨ì–´ ë‹¨ìœ„ ëª¨ë¸
â”‚   â””â”€â”€ sentence_level/          # ë¬¸ì¥ ë‹¨ìœ„ ëª¨ë¸
â””â”€â”€ logs/                        # í›ˆë ¨ ë¡œê·¸
```

## ğŸ”§ ì„¤ì¹˜ ë° ì„¤ì •

### 1. ì €ì¥ì†Œ í´ë¡ 
```bash
git clone https://github.com/your-repo/korean-sign-recognition.git
cd korean-sign-recognition
```

### 2. ê°€ìƒí™˜ê²½ ì„¤ì •
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ë˜ëŠ”
venv\Scripts\activate     # Windows
```

### 3. ì˜ì¡´ì„± ì„¤ì¹˜
```bash
pip install -r requirements.txt
```

### 4. ë°ì´í„° ì¤€ë¹„
AIHubì—ì„œ í•œêµ­ ìˆ˜ì–´ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  `data/aihub_sign_data/` í´ë”ì— ì••ì¶• í•´ì œ:

```
data/aihub_sign_data/
â”œâ”€â”€ video1.mp4
â”œâ”€â”€ video1_morpheme.json
â”œâ”€â”€ video2.mp4
â”œâ”€â”€ video2_morpheme.json
...
```

## ğŸ¯ ì‚¬ìš© ë°©ë²•

### ë‹¨ê³„ 1: ë°ì´í„° ì „ì²˜ë¦¬

AIHub ë°ì´í„°ë¥¼ ëª¨ë¸ í›ˆë ¨ìš©ìœ¼ë¡œ ì „ì²˜ë¦¬:

```bash
python main.py preprocess \
    --data_dir ./data/aihub_sign_data \
    --output_dir ./data/processed_data \
    --sequence_length 32 \
    --train_ratio 0.8
```

**ì£¼ìš” ì˜µì…˜:**
- `--data_dir`: AIHub ì›ë³¸ ë°ì´í„° ê²½ë¡œ
- `--output_dir`: ì „ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ê²½ë¡œ
- `--sequence_length`: ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸: 32)
- `--train_ratio`: í›ˆë ¨/ê²€ì¦ ë¶„í•  ë¹„ìœ¨ (ê¸°ë³¸: 0.8)

### ë‹¨ê³„ 2: ë‹¨ì–´ ë‹¨ìœ„ ëª¨ë¸ í›ˆë ¨

OpenHands ëª¨ë¸ì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ì‚¬ì „í›ˆë ¨:

```bash
python main.py train \
    --processed_data_dir ./data/processed_data \
    --model_save_dir ./models/word_level \
    --batch_size 16 \
    --learning_rate 1e-4 \
    --num_epochs 50
```

**ì£¼ìš” ì˜µì…˜:**
- `--batch_size`: ë°°ì¹˜ í¬ê¸° (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)
- `--learning_rate`: í•™ìŠµë¥  (ê¸°ë³¸: 1e-4)
- `--num_epochs`: í›ˆë ¨ ì—í¬í¬ ìˆ˜
- `--d_model`: ëª¨ë¸ ì°¨ì› (ê¸°ë³¸: 256)
- `--n_heads`: ì–´í…ì…˜ í—¤ë“œ ìˆ˜ (ê¸°ë³¸: 8)

### ë‹¨ê³„ 3: ë¬¸ì¥ ë‹¨ìœ„ ëª¨ë¸ í›ˆë ¨

ìì—°ì–´ ë¬¸ì¥ ì–´ë…¸í…Œì´ì…˜ ì¤€ë¹„:

```python
# text_annotations.json ì˜ˆì‹œ
{
  "video_001": {
    "sentence": "ì•ˆë…•í•˜ì„¸ìš”. ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤.",
    "keywords": ["ì•ˆë…•", "ë§Œë‚˜ë‹¤", "ë°˜ê°‘ë‹¤"]
  },
  "video_002": {
    "sentence": "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”.",
    "keywords": ["ì˜¤ëŠ˜", "ë‚ ì”¨", "ì¢‹ë‹¤"]
  }
}
```

Encoder-Decoder ëª¨ë¸ í›ˆë ¨:

```bash
python -c "
from encoder_decoder_model import train_sign_to_text_model
train_sign_to_text_model(
    sign_data_dir='./data/processed_data',
    text_annotations_path='./data/text_annotations.json',
    pretrained_encoder_path='./models/word_level/best_model.pt',
    save_dir='./models/sentence_level'
)
"
```

### ë‹¨ê³„ 4: ì¶”ë¡  ë° í…ŒìŠ¤íŠ¸

í›ˆë ¨ëœ ëª¨ë¸ë¡œ ì¶”ë¡ :

```bash
python main.py inference \
    --model_path ./models/sentence_level/best_model.pt \
    --processed_data_dir ./data/processed_data
```

## ğŸ’» ì½”ë“œ ì‚¬ìš© ì˜ˆì‹œ

### ê¸°ë³¸ ì¶”ë¡ 
```python
from encoder_decoder_model import SignToTextModel
import torch

# ëª¨ë¸ ë¡œë“œ
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = SignToTextModel.load_from_checkpoint('./models/sentence_level/best_model.pt')
model.to(device)
model.eval()

# ìˆ˜ì–´ íŠ¹ì§• ì…ë ¥ (ì˜ˆì‹œ)
sign_features = torch.randn(1, 32, 144).to(device)  # (ë°°ì¹˜, ì‹œí€€ìŠ¤, íŠ¹ì§•)

# í…ìŠ¤íŠ¸ ìƒì„±
generated_texts = model.generate_text(sign_features, max_length=50)
print(f"ë²ˆì—­ ê²°ê³¼: {generated_texts[0]}")
```

### ì‹¤ì‹œê°„ ëŒ€í™”í˜• ë²ˆì—­ê¸°
```python
from advanced_sign_to_text import InteractiveSignTranslator
import numpy as np

# ë²ˆì—­ê¸° ì´ˆê¸°í™”
translator = InteractiveSignTranslator('./models/sentence_level/best_model.pt')

# ìˆ˜ì–´ ì‹œí€€ìŠ¤ ë²ˆì—­
sign_sequence = np.random.randn(32, 144)  # ì‹¤ì œë¡œëŠ” MediaPipeì—ì„œ ì¶”ì¶œ
result = translator.translate_sign_sequence(
    sign_sequence,
    use_beam_search=True,
    use_context=True
)

print(f"ë²ˆì—­: {result['translation']}")
print(f"ì‹ ë¢°ë„: {result['confidence']:.2f}")
print(f"ë¬¸ë§¥: {result['context']}")
print(f"ëŒ€ì•ˆ: {result['suggestions']}")
```

### ì›¹ìº  ì‹¤ì‹œê°„ ë²ˆì—­
```python
import cv2
import mediapipe as mp
from data_preprocessor import SignLanguagePreprocessor

# MediaPipe ì´ˆê¸°í™”
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.7)

# ì›¹ìº  ì‹œì‘
cap = cv2.VideoCapture(0)
translator = InteractiveSignTranslator('./models/sentence_level/best_model.pt')

sequence_buffer = []
sequence_length = 32

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    
    # ì† ëœë“œë§ˆí¬ ì¶”ì¶œ
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(rgb_frame)
    
    if results.multi_hand_landmarks:
        # íŠ¹ì§• ì¶”ì¶œ (ì‹¤ì œ êµ¬í˜„ í•„ìš”)
        features = extract_features_from_landmarks(results.multi_hand_landmarks)
        sequence_buffer.append(features)
        
        # ì¶©ë¶„í•œ í”„ë ˆì„ì´ ëª¨ì´ë©´ ë²ˆì—­
        if len(sequence_buffer) >= sequence_length:
            sign_features = np.array(sequence_buffer[-sequence_length:])
            result = translator.translate_sign_sequence(sign_features)
            
            # í™”ë©´ì— ë²ˆì—­ ê²°ê³¼ í‘œì‹œ
            cv2.putText(frame, result['translation'], (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            cv2.putText(frame, f"ì‹ ë¢°ë„: {result['confidence']:.2f}", 
                       (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)
    
    cv2.imshow('ìˆ˜ì–´ ë²ˆì—­ê¸°', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```

## ğŸ” í‰ê°€ ë° ì„±ëŠ¥ ì¸¡ì •

### ë²ˆì—­ í’ˆì§ˆ í‰ê°€
```python
from nltk.translate.bleu_score import corpus_bleu
import numpy as np

def evaluate_model(model, test_loader):
    predictions = []
    references = []
    
    model.eval()
    with torch.no_grad():
        for batch in test_loader:
            # ì˜ˆì¸¡
            pred_texts = model.generate_text(batch['sign_features'])
            predictions.extend(pred_texts)
            
            # ì°¸ì¡° í…ìŠ¤íŠ¸
            ref_texts = batch['text_string']
            references.extend([[ref.split()] for ref in ref_texts])
    
    # BLEU ìŠ¤ì½”ì–´ ê³„ì‚°
    bleu_score = corpus_bleu(references, [pred.split() for pred in predictions])
    print(f"BLEU Score: {bleu_score:.4f}")
    
    return bleu_score, predictions, references
```

### ì‹¤ì‹œê°„ ì„±ëŠ¥ ì¸¡ì •
```python
import time

def benchmark_inference_speed(model, test_features, num_runs=100):
    model.eval()
    times = []
    
    for _ in range(num_runs):
        start_time = time.time()
        with torch.no_grad():
            _ = model.generate_text(test_features)
        end_time = time.time()
        times.append(end_time - start_time)
    
    avg_time = np.mean(times)
    fps = 1.0 / avg_time
    
    print(f"í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_time:.4f}ì´ˆ")
    print(f"ì´ˆë‹¹ í”„ë ˆì„ ìˆ˜: {fps:.2f} FPS")
    
    return avg_time, fps
```

## âš™ï¸ ê³ ê¸‰ ì„¤ì •

### GPU ì„¤ì •
```python
# CUDA GPU ì‚¬ìš©
export CUDA_VISIBLE_DEVICES=0

# Intel GPU ì‚¬ìš© (ì§€ì›ë˜ëŠ” ê²½ìš°)
export USE_INTEL_GPU=1
```

### ëª¨ë¸ ìµœì í™”
```python
# ëª¨ë¸ ì–‘ìí™” (ì¶”ë¡  ì†ë„ í–¥ìƒ)
from torch.quantization import quantize_dynamic

model_quantized = quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)
```

### ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
```python
# ë™ì  ë°°ì¹˜ í¬ê¸°
def collate_fn(batch):
    # ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ë¡œ ì •ë ¬í•˜ì—¬ íŒ¨ë”© ìµœì†Œí™”
    batch.sort(key=lambda x: x['sign_features'].size(0), reverse=True)
    return default_collate(batch)

train_loader = DataLoader(
    dataset, 
    batch_size=16, 
    collate_fn=collate_fn,
    num_workers=4,
    pin_memory=True
)
```

## ğŸ› ë¬¸ì œ í•´ê²°

### ìì£¼ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜

**1. CUDA ë©”ëª¨ë¦¬ ë¶€ì¡±**
```bash
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
python main.py train --batch_size 8

# ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ì‚¬ìš©
python main.py train --batch_size 4 --gradient_accumulation_steps 4
```

**2. JSON íŒŒì¼ ë§¤ì¹­ ì‹¤íŒ¨**
```bash
# íŒŒì¼ëª… íŒ¨í„´ í™•ì¸
ls data/aihub_sign_data/*.json | head -5
ls data/aihub_sign_data/*.mp4 | head -5

# ë¡œê·¸ ë ˆë²¨ì„ DEBUGë¡œ ì„¤ì •í•˜ì—¬ ìì„¸í•œ ì •ë³´ í™•ì¸
python main.py preprocess --log_level DEBUG
```

**3. Intel GPU ì¸ì‹ ì‹¤íŒ¨**
```bash
# Intel Extension ì„¤ì¹˜ í™•ì¸
pip install intel_extension_for_pytorch

# CPU í´ë°± ì‚¬ìš©
export USE_INTEL_GPU=0
```

### ì„±ëŠ¥ ìµœì í™” íŒ

1. **ë°ì´í„° ë¡œë”© ìµœì í™”**: `num_workers=4`, `pin_memory=True` ì‚¬ìš©
2. **Mixed Precision**: AMP ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½ ë° ì†ë„ í–¥ìƒ
3. **ëª¨ë¸ ë³‘ë ¬í™”**: í° ëª¨ë¸ì˜ ê²½ìš° DataParallel ì‚¬ìš©
4. **ìºì‹œ í™œìš©**: ì „ì²˜ë¦¬ëœ íŠ¹ì§•ì„ ë””ìŠ¤í¬ì— ìºì‹œ

## ğŸ™ ê°ì‚¬ì˜ ë§

- [AIHub](https://aihub.or.kr/) - í•œêµ­ ìˆ˜ì–´ ë°ì´í„°ì…‹ ì œê³µ
- [OpenHands](https://github.com/AI4Bharat/OpenHands) - ìˆ˜ì–´ ì¸ì‹ ê¸°ë°˜ ëª¨ë¸
- [MediaPipe](https://mediapipe.dev/) - ì† ëœë“œë§ˆí¬ ì¶”ì¶œ
- [Transformers](https://huggingface.co/transformers/) - ì‚¬ì „í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸

---

**ğŸ’¡ Tip**: ë” ìì„¸í•œ ê¸°ìˆ  ë¬¸ì„œì™€ ì˜ˆì œëŠ” [Wiki í˜ì´ì§€](https://github.com/your-repo/korean-sign-recognition/wiki)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”!