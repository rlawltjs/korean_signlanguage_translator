#!/usr/bin/env python3
"""
í¬ì¦ˆ ì¶”ì • ì„œë²„: ì„ì˜ í¬ê¸° ì´ë¯¸ì§€ ìˆ˜ì‹  â†’ YOLOv8n ì‚¬ëŒ 1ëª… ê²€ì¶œ â†’ 288x384 ë¦¬ì‚¬ì´ì¦ˆ â†’ RTMW í¬ì¦ˆ ì¶”ì • â†’ ê²°ê³¼ ë°˜í™˜
ìˆ˜ì–´ ì¸ì‹ ê¸°ëŠ¥ ì¶”ê°€: MediaPipe ê¸°ë°˜ ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸ ì§€ì› (OpenHands Transformer ëª¨ë¸)
"""

import cv2
import numpy as np
import time
import json
import argparse
from pathlib import Path
import logging
from typing import Optional, Tuple, Dict, Any, List
from collections import deque
import os

# Flask ì›¹ ì„œë²„
from flask import Flask, request, jsonify
from werkzeug.utils import secure_filename
import threading
import queue

# MMPose ê´€ë ¨
from mmpose.apis import init_model, inference_topdown
import torch
import torch.nn as nn

# YOLOv8 ê´€ë ¨
from ultralytics import YOLO

# MediaPipe ê´€ë ¨ (ìˆ˜ì–´ ì¸ì‹ìš©)
import mediapipe as mp

# Transformer ê´€ë ¨ (OpenHands ëª¨ë¸ìš©)
try:
    from transformers import AutoModel, AutoConfig
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    print("âš ï¸ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. pip install transformers ì‹¤í–‰í•˜ì„¸ìš”.")
    TRANSFORMERS_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PersonDetector:
    """YOLOv11ì„ ì‚¬ìš©í•œ ì‚¬ëŒ 1ëª… ê²€ì¶œê¸° (Intel XPU ì§€ì›)"""
    
    def __init__(self, device: str = "auto", model_size: str = "n"):
        self.device = self._determine_device(device)
        self.model_size = model_size
        
        # Intel Extension for PyTorch import (XPU ì§€ì›)
        try:
            if self.device == "xpu":
                import intel_extension_for_pytorch as ipex
                print(f"ğŸ”§ Intel Extension for PyTorch ë¡œë”©ë¨")
        except ImportError:
            if self.device == "xpu":
                print(f"âš ï¸ Intel Extension for PyTorch ì—†ìŒ - CPUë¡œ í´ë°±")
                self.device = "cpu"
        
        print(f"ğŸ”§ YOLOv11{model_size} ëª¨ë¸ ë¡œë”© ì¤‘... (ë””ë°”ì´ìŠ¤: {self.device})")
        start_time = time.time()
        
        try:
            # YOLOv11 ëª¨ë¸ ë¡œë”© (YOLOv8 ëŒ€ë¹„ ë” ë¹ ë¥´ê³  ì •í™•í•¨)
            model_name = f'yolo11{model_size}.pt'
            self.yolo_model = YOLO(model_name)
            
            # XPU ë˜ëŠ” ë‹¤ë¥¸ ë””ë°”ì´ìŠ¤ ì„¤ì •
            if self.device != 'cpu':
                self.yolo_model.to(self.device)
            
            init_time = time.time() - start_time
            print(f"âœ… YOLOv11{model_size} ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {init_time:.2f}ì´ˆ")
            print(f"   - ì„±ëŠ¥: YOLOv8 ëŒ€ë¹„ ë” ë¹ ë¥¸ ì¶”ë¡  ì†ë„ì™€ ë†’ì€ ì •í™•ë„")
            print(f"   - íŒŒë¼ë¯¸í„°: YOLOv8 ëŒ€ë¹„ 22% ì ì€ íŒŒë¼ë¯¸í„°ë¡œ ë” ë†’ì€ mAP")
            
        except Exception as e:
            print(f"âŒ YOLOv11{model_size} ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            print(f"ğŸ”„ YOLOv8nìœ¼ë¡œ í´ë°±...")
            try:
                self.yolo_model = YOLO('yolov8n.pt')
                if self.device != 'cpu':
                    self.yolo_model.to(self.device)
                init_time = time.time() - start_time
                print(f"âœ… YOLOv8n í´ë°± ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {init_time:.2f}ì´ˆ")
            except Exception as e2:
                print(f"âŒ í´ë°± ëª¨ë¸ë„ ì‹¤íŒ¨: {e2}")
                raise
    
    def _determine_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê²°ì • (XPU ìš°ì„ )"""
        if device == "auto":
            # 1. XPU (Intel Arc GPU) í™•ì¸
            try:
                if torch.xpu.is_available():
                    print(f"ğŸ® Intel Arc GPU (XPU) ê°ì§€ë¨")
                    return "xpu"
            except:
                pass
            
            # 2. CUDA í™•ì¸  
            if torch.cuda.is_available():
                print(f"ğŸš€ NVIDIA GPU (CUDA) ê°ì§€ë¨")
                return "cuda"
            
            # 3. CPU í´ë°±
            print(f"ğŸ’» CPU ì‚¬ìš©")
            return "cpu"
        else:
            # ì‚¬ìš©ì ì§€ì • ë””ë°”ì´ìŠ¤ ê²€ì¦
            if device == "xpu":
                try:
                    if not torch.xpu.is_available():
                        print("âš ï¸ XPU ë¯¸ì‚¬ìš© ê°€ëŠ¥ - CPUë¡œ í´ë°±")
                        return "cpu"
                except:
                    print("âš ï¸ XPU í™•ì¸ ì‹¤íŒ¨ - CPUë¡œ í´ë°±")
                    return "cpu"
            elif device == "cuda":
                if not torch.cuda.is_available():
                    print("âš ï¸ CUDA ë¯¸ì‚¬ìš© ê°€ëŠ¥ - CPUë¡œ í´ë°±")
                    return "cpu"
            
            return device
    
    def detect_best_person(self, image: np.ndarray, conf_threshold: float = 0.5) -> Optional[List[float]]:
        """ì´ë¯¸ì§€ì—ì„œ ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ ì‚¬ëŒ 1ëª… ê²€ì¶œ
        
        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€ (BGR)
            conf_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’
            
        Returns:
            Optional[List[float]]: [x1, y1, x2, y2, confidence] í˜•íƒœì˜ ë°”ìš´ë”© ë°•ìŠ¤ ë˜ëŠ” None
        """
        try:
            # YOLOv8 ì¶”ë¡ 
            results = self.yolo_model(image, verbose=False)
            
            best_person = None
            best_confidence = 0.0
            
            for result in results:
                boxes = result.boxes
                if boxes is not None:
                    # í´ë˜ìŠ¤ 0ì€ 'person'
                    person_mask = boxes.cls == 0
                    conf_mask = boxes.conf >= conf_threshold
                    valid_mask = person_mask & conf_mask
                    
                    if valid_mask.any():
                        valid_boxes = boxes.xyxy[valid_mask]  # x1, y1, x2, y2
                        valid_confs = boxes.conf[valid_mask]
                        
                        # ê°€ì¥ ì‹ ë¢°ë„ê°€ ë†’ì€ ì‚¬ëŒ ì„ íƒ
                        for box, conf in zip(valid_boxes, valid_confs):
                            conf_value = float(conf.cpu())
                            if conf_value > best_confidence:
                                best_confidence = conf_value
                                x1, y1, x2, y2 = box.cpu().numpy()
                                best_person = [float(x1), float(y1), float(x2), float(y2), conf_value]
            
            return best_person
            
        except Exception as e:
            logger.error(f"âŒ ì‚¬ëŒ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return None

class RTMWPoseEstimator:
    """RTMW í¬ì¦ˆ ì¶”ì •ê¸°"""
    
    def __init__(self, 
                 rtmw_config: str,
                 rtmw_checkpoint: str,
                 device: str = "auto"):
        
        self.rtmw_config = rtmw_config
        self.rtmw_checkpoint = rtmw_checkpoint
        self.device = self._determine_device(device)
        
        # PyTorch ë³´ì•ˆ ì„¤ì •
        self.original_load = torch.load
        torch.load = lambda *args, **kwargs: self.original_load(
            *args, **kwargs, weights_only=False
        ) if 'weights_only' not in kwargs else self.original_load(*args, **kwargs)
        
        print(f"ğŸ”§ RTMW í¬ì¦ˆ ëª¨ë¸ ë¡œë”© ì¤‘... (ë””ë°”ì´ìŠ¤: {self.device})")
        start_time = time.time()
        
        try:
            self.pose_model = init_model(
                config=self.rtmw_config,
                checkpoint=self.rtmw_checkpoint,
                device=self.device
            )
            
            init_time = time.time() - start_time
            print(f"âœ… RTMW í¬ì¦ˆ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {init_time:.2f}ì´ˆ")
            
        except Exception as e:
            print(f"âŒ {self.device} í¬ì¦ˆ ëª¨ë¸ ì‹¤íŒ¨: {e}")
            if self.device != 'cpu':
                print(f"ğŸ”„ CPUë¡œ í´ë°±...")
                self.device = 'cpu'
                
                self.pose_model = init_model(
                    config=self.rtmw_config,
                    checkpoint=self.rtmw_checkpoint,
                    device='cpu'
                )
                
                init_time = time.time() - start_time
                print(f"âœ… CPU í¬ì¦ˆ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {init_time:.2f}ì´ˆ")
            else:
                raise
    
    def _determine_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê²°ì •"""
        if device == "auto":
            # XPU í™•ì¸
            try:
                if torch.xpu.is_available():
                    return "xpu"
            except:
                pass
            
            # CUDA í™•ì¸
            if torch.cuda.is_available():
                return "cuda"
            
            return "cpu"
        else:
            # ì‚¬ìš©ì ì§€ì • ë””ë°”ì´ìŠ¤ ê²€ì¦
            if device == "xpu":
                try:
                    if not torch.xpu.is_available():
                        print("âš ï¸ XPU ë¯¸ì‚¬ìš© ê°€ëŠ¥ - CPUë¡œ í´ë°±")
                        return "cpu"
                except:
                    print("âš ï¸ XPU í™•ì¸ ì‹¤íŒ¨ - CPUë¡œ í´ë°±")
                    return "cpu"
            elif device == "cuda":
                if not torch.cuda.is_available():
                    print("âš ï¸ CUDA ë¯¸ì‚¬ìš© ê°€ëŠ¥ - CPUë¡œ í´ë°±")
                    return "cpu"
            
            return device
    
    def crop_and_resize_person(self, image: np.ndarray, bbox: List[float], 
                              target_size: Tuple[int, int] = (288, 384)) -> np.ndarray:
        """ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ëŒ ì˜ì—­ì„ í¬ë¡­í•˜ê³  288x384ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        
        Args:
            image: ì›ë³¸ ì´ë¯¸ì§€
            bbox: [x1, y1, x2, y2, confidence] ë°”ìš´ë”© ë°•ìŠ¤
            target_size: ëª©í‘œ í¬ê¸° (width, height)
            
        Returns:
            np.ndarray: ë¦¬ì‚¬ì´ì¦ˆëœ 288x384 ì´ë¯¸ì§€
        """
        try:
            h, w = image.shape[:2]
            x1, y1, x2, y2 = bbox[:4]
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ì •ìˆ˜í™” ë° ê²½ê³„ ê²€ì‚¬
            x1 = max(0, int(x1))
            y1 = max(0, int(y1))
            x2 = min(w, int(x2))
            y2 = min(h, int(y2))
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ìœ íš¨ì„± ê²€ì‚¬
            if x2 <= x1 or y2 <= y1:
                logger.warning("âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ë°”ìš´ë”© ë°•ìŠ¤, ì „ì²´ ì´ë¯¸ì§€ ì‚¬ìš©")
                crop = image.copy()
            else:
                # ì‚¬ëŒ ì˜ì—­ í¬ë¡­
                crop = image[y1:y2, x1:x2]
            
            # 288x384ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (RTMW ì…ë ¥ í¬ê¸°)
            resized = cv2.resize(crop, target_size, interpolation=cv2.INTER_LINEAR)
            
            return resized
            
        except Exception as e:
            logger.error(f"âŒ í¬ë¡­ ë° ë¦¬ì‚¬ì´ì¦ˆ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ì‹œ ì „ì²´ ì´ë¯¸ì§€ë¥¼ ë¦¬ì‚¬ì´ì¦ˆ
            return cv2.resize(image, target_size, interpolation=cv2.INTER_LINEAR)
    
    def estimate_pose_on_crop(self, crop_image: np.ndarray) -> Tuple[np.ndarray, np.ndarray, float]:
        """í¬ë¡­ëœ ì´ë¯¸ì§€ì—ì„œ í¬ì¦ˆ ì¶”ì •"""
        try:
            start_time = time.time()
            
            # í¬ë¡­ ì´ë¯¸ì§€ ì „ì²´ë¥¼ ë°”ìš´ë”©ë°•ìŠ¤ë¡œ ì‚¬ìš© (288x384)
            h, w = crop_image.shape[:2]
            full_bbox = [0, 0, w, h]
            
            # MMPose ì¶”ë¡ 
            results = inference_topdown(
                model=self.pose_model,
                img=crop_image,
                bboxes=[full_bbox],
                bbox_format='xyxy'
            )
            
            pose_time = time.time() - start_time
            
            if results and len(results) > 0:
                keypoints = results[0].pred_instances.keypoints[0]
                scores = results[0].pred_instances.keypoint_scores[0]
                
                if isinstance(keypoints, torch.Tensor):
                    keypoints = keypoints.cpu().numpy()
                if isinstance(scores, torch.Tensor):
                    scores = scores.cpu().numpy()
                
                return keypoints, scores, pose_time
            else:
                return np.zeros((133, 2)), np.zeros(133), pose_time
                
        except Exception as e:
            logger.error(f"âŒ í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {e}")
            return np.zeros((133, 2)), np.zeros(133), 0.0

class OpenHandsTransformerModel(nn.Module):
    """OpenHands ê¸°ë°˜ Transformer ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸"""
    
    def __init__(self, 
                 input_size: int = 144,
                 hidden_size: int = 256,  # 512 -> 256ìœ¼ë¡œ ë³€ê²½
                 num_classes: int = 100,
                 num_heads: int = 8,
                 num_layers: int = 6,
                 dropout: float = 0.1,
                 max_seq_length: int = 64):
        super().__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_classes = num_classes
        self.max_seq_length = max_seq_length
        
        # ì…ë ¥ ì„ë² ë”© (feature_projectionìœ¼ë¡œ ì´ë¦„ ë³€ê²½)
        self.feature_projection = nn.Linear(input_size, hidden_size)
        
        # ìœ„ì¹˜ ì¸ì½”ë”©
        self.positional_encoding = self._create_positional_encoding(max_seq_length, hidden_size)
        
        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_size,
            nhead=num_heads,
            dim_feedforward=hidden_size * 4,  # 2048 -> 1024
            dropout=dropout,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # ë¶„ë¥˜ í—¤ë“œ (256 -> 128 -> 14 êµ¬ì¡°ë¡œ ë³€ê²½)
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, 128),  # 256 -> 128
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, num_classes)   # 128 -> num_classes
        )
    
    def _create_positional_encoding(self, max_len: int, d_model: int) -> torch.Tensor:
        """ìœ„ì¹˜ ì¸ì½”ë”© ìƒì„±"""
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        
        return pe
    
    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            x: (batch_size, seq_len, input_size)
            attention_mask: (batch_size, seq_len) - True for valid positions
        
        Returns:
            torch.Tensor: (batch_size, num_classes)
        """
        batch_size, seq_len, _ = x.shape
        
        # ì…ë ¥ ì„ë² ë”©
        x = self.feature_projection(x)  # (batch_size, seq_len, hidden_size)
        
        # ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€
        pos_encoding = self.positional_encoding[:, :seq_len, :].to(x.device)
        x = x + pos_encoding
        
        # Transformer Encoder
        if attention_mask is not None:
            # PyTorch TransformerëŠ” Trueì¸ ìœ„ì¹˜ë¥¼ mask(ë¬´ì‹œ)ë¡œ ì²˜ë¦¬
            src_key_padding_mask = ~attention_mask
        else:
            src_key_padding_mask = None
        
        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)
        
        # Global average pooling
        if attention_mask is not None:
            # ìœ íš¨í•œ í† í°ë“¤ë§Œ í‰ê· 
            mask = attention_mask.unsqueeze(-1).float()  # (batch_size, seq_len, 1)
            x = (x * mask).sum(dim=1) / mask.sum(dim=1)  # (batch_size, hidden_size)
        else:
            # ì „ì²´ ì‹œí€€ìŠ¤ í‰ê· 
            x = x.mean(dim=1)  # (batch_size, hidden_size)
        
        # ë¶„ë¥˜
        logits = self.classifier(x)  # (batch_size, num_classes)
        
        return logits

class SignLanguageRecognizer:
    """ìˆ˜ì–´ ì¸ì‹ê¸° - MediaPipe + OpenHands Transformer ëª¨ë¸ ê¸°ë°˜"""
    
    def __init__(self, sign_model_path: str = None, sequence_length: int = 32):
        self.sign_model_path = sign_model_path
        self.sequence_length = sequence_length
        self.sign_model = None
        self.device = self._setup_device()
        
        # MediaPipe ì´ˆê¸°í™”
        print(f"ğŸ”§ MediaPipe ì´ˆê¸°í™” ì¤‘...")
        self.mp_hands = mp.solutions.hands
        self.mp_pose = mp.solutions.pose
        
        self.hands = self.mp_hands.Hands(
            static_image_mode=True,
            max_num_hands=2,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
        )
        
        self.pose = self.mp_pose.Pose(
            static_image_mode=True,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
        )
        
        # ìˆ˜ì–´ ëª¨ë¸ ë¡œë“œ ì‹œë„
        if sign_model_path and os.path.exists(sign_model_path):
            self._load_transformer_model()
        else:
            print(f"âš ï¸ ìˆ˜ì–´ ëª¨ë¸ ê²½ë¡œ ì—†ìŒ ë˜ëŠ” íŒŒì¼ ì—†ìŒ: {sign_model_path}")
            print(f"   - íŠ¹ì§• ì¶”ì¶œë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        print(f"âœ… ìˆ˜ì–´ ì¸ì‹ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _setup_device(self):
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if torch.cuda.is_available():
            return torch.device("cuda")
        elif hasattr(torch, 'xpu') and torch.xpu.is_available():
            return torch.device("xpu")
        else:
            return torch.device("cpu")
    
    def _load_transformer_model(self):
        """OpenHands Transformer ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸ ë¡œë“œ"""
        try:
            print(f"ğŸ”§ OpenHands Transformer ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸ ë¡œë”© ì¤‘: {self.sign_model_path}")
            
            # PyTorch ëª¨ë¸ ë¡œë“œ
            checkpoint = torch.load(self.sign_model_path, map_location=self.device)
            
            # ì²´í¬í¬ì¸íŠ¸ì˜ ëª¨ë¸ ì„¤ì • í™•ì¸
            if 'model_config' in checkpoint:
                config = checkpoint['model_config']
                model_config = {
                    'input_size': config.get('input_dim', 144),
                    'hidden_size': config.get('d_model', 256),
                    'num_classes': config.get('vocab_size', 14),
                    'num_heads': 8,  # ê¸°ë³¸ê°’ ì‚¬ìš©
                    'num_layers': 6,  # ê¸°ë³¸ê°’ ì‚¬ìš©
                    'dropout': 0.1,   # ê¸°ë³¸ê°’ ì‚¬ìš©
                    'max_seq_length': config.get('max_seq_length', 32)
                }
                
                print(f"ğŸ“Š ëª¨ë¸ ì„¤ì • ë¡œë“œë¨:")
                print(f"   - ì…ë ¥ í¬ê¸°: {model_config['input_size']}")
                print(f"   - Hidden í¬ê¸°: {model_config['hidden_size']}")
                print(f"   - í´ë˜ìŠ¤ ìˆ˜: {model_config['num_classes']}")
                print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {model_config['max_seq_length']}")
            else:
                raise ValueError("ì²´í¬í¬ì¸íŠ¸ì— model_configê°€ ì—†ìŠµë‹ˆë‹¤.")

            # OpenHands Transformer ëª¨ë¸ ìƒì„±
            self.sign_model = OpenHandsTransformerModel(
                input_size=model_config['input_size'],
                hidden_size=model_config['hidden_size'],
                num_classes=model_config['num_classes'],
                num_heads=model_config['num_heads'],
                num_layers=model_config['num_layers'],
                dropout=model_config['dropout'],
                max_seq_length=model_config['max_seq_length']
            )
            
            # ê°€ì¤‘ì¹˜ ë¡œë“œ
            if 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint.get('model_state_dict', checkpoint)
            
            # í‚¤ ì´ë¦„ ë§¤í•‘
            mapped_state_dict = {}
            for k, v in state_dict.items():
                k = k.replace('module.', '')
                k = k.replace('input_projection.', 'feature_projection.')
                mapped_state_dict[k] = v
                
            self.sign_model.load_state_dict(mapped_state_dict)
            self.sign_model.to(self.device)
            self.sign_model.eval()
            
            # â­ í•µì‹¬ ìˆ˜ì • ë¶€ë¶„: í´ë˜ìŠ¤ ì´ë¦„ì„ ì§ì ‘ ì •ì˜
            if 'class_names' in checkpoint:
                self.class_names = checkpoint['class_names']
                print(f"ğŸ“š ì²´í¬í¬ì¸íŠ¸ì—ì„œ í´ë˜ìŠ¤ ì´ë¦„ ë¡œë“œë¨")
            else:
                # ìˆ˜ë™ìœ¼ë¡œ í´ë˜ìŠ¤ ì´ë¦„ ë§¤í•‘ ì •ì˜
                self.class_names = [
                    "<PAD>",           # class_0
                    "<UNK>",           # class_1
                    "<SOS>",           # class_2
                    "<EOS>",           # class_3
                    "ê°ì‚¬í•˜ë‹¤",         # class_4
                    "ê°ì‚¬í•©ë‹ˆë‹¤",       # class_5
                    "ë‚˜",              # class_6
                    "ë§Œë‚˜ë‹¤",          # class_7
                    "ë§Œë‚˜ì„œ",          # class_8
                    # "ë¯¸ì•ˆí•©ë‹ˆë‹¤",       # class_9
                    "ë°˜ê°‘ë‹¤",          # class_10
                    "ì•ˆë…•í•˜ì„¸ìš”",       # class_11
                    "ì €",              # class_12
                    # "ì£„ì†¡í•˜ë‹¤"         # class_13
                ]
                print(f"ğŸ“š ìˆ˜ë™ìœ¼ë¡œ ì •ì˜ëœ í´ë˜ìŠ¤ ì´ë¦„ ì‚¬ìš©")
                
                print(f"âœ… OpenHands Transformer ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                print(f"   - ëª¨ë¸ íƒ€ì…: Transformer (OpenHands fine-tuned)")
                print(f"   - í´ë˜ìŠ¤ ìˆ˜: {len(self.class_names)}")
                print(f"   - ë””ë°”ì´ìŠ¤: {self.device}")
                print(f"   - íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in self.sign_model.parameters())}")
                
                # í´ë˜ìŠ¤ ì´ë¦„ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                print(f"ğŸ·ï¸ í´ë˜ìŠ¤ ë§¤í•‘:")
                for i, name in enumerate(self.class_names):
                    print(f"   class_{i}: {name}")
            
        except Exception as e:
            print(f"âŒ OpenHands Transformer ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"   - ì²´í¬í¬ì¸íŠ¸ í˜•ì‹ì„ í™•ì¸í•˜ì„¸ìš”.")
            self.sign_model = None
    
    def extract_features_from_image(self, image: np.ndarray) -> Optional[np.ndarray]:
        """ì´ë¯¸ì§€ì—ì„œ MediaPipe íŠ¹ì§• ì¶”ì¶œ"""
        try:
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # ì† ëœë“œë§ˆí¬ ì¶”ì¶œ
            hand_results = self.hands.process(rgb_image)
            hand_features = self._extract_hand_features(hand_results)
            
            # í¬ì¦ˆ ëœë“œë§ˆí¬ ì¶”ì¶œ
            pose_results = self.pose.process(rgb_image)
            pose_features = self._extract_pose_features(pose_results)
            
            if hand_features is not None and pose_features is not None:
                combined_features = np.concatenate([hand_features, pose_features])
                return combined_features
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_hand_features(self, results) -> Optional[np.ndarray]:
        """ì† ëœë“œë§ˆí¬ ì¶”ì¶œ"""
        landmarks = []
        
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                hand_points = []
                for landmark in hand_landmarks.landmark:
                    hand_points.extend([landmark.x, landmark.y, landmark.z])
                landmarks.extend(hand_points)
        
        # 126ì°¨ì›ìœ¼ë¡œ íŒ¨ë”©/ìë¥´ê¸° (ì–‘ì† 21*3*2 = 126)
        if len(landmarks) == 0:
            return np.zeros(126, dtype=np.float32)
        elif len(landmarks) == 63:  # í•œ ì†ë§Œ ê²€ì¶œëœ ê²½ìš°
            landmarks.extend([0] * 63)
        elif len(landmarks) > 126:
            landmarks = landmarks[:126]
        
        return np.array(landmarks, dtype=np.float32)
    
    def _extract_pose_features(self, results) -> Optional[np.ndarray]:
        """í¬ì¦ˆ ëœë“œë§ˆí¬ ì¶”ì¶œ (ìƒì²´ ìœ„ì£¼)"""
        if results.pose_landmarks:
            # ì¤‘ìš”í•œ ìƒì²´ í¬ì¦ˆ í¬ì¸íŠ¸
            important_indices = [11, 12, 13, 14, 15, 16]  # ì–´ê¹¨, íŒ”ê¿ˆì¹˜, ì†ëª©
            landmarks = []
            
            for idx in important_indices:
                landmark = results.pose_landmarks.landmark[idx]
                landmarks.extend([landmark.x, landmark.y, landmark.z])
            
            return np.array(landmarks, dtype=np.float32)
        else:
            return np.zeros(18, dtype=np.float32)  # 6 * 3 = 18
    
    def predict_sign_sequence(self, feature_sequence: List[np.ndarray], confidence_threshold: float = 0.6) -> Dict[str, Any]:
        """íŠ¹ì§• ì‹œí€€ìŠ¤ì—ì„œ ìˆ˜ì–´ ì˜ˆì¸¡ (Transformer ê¸°ë°˜)"""
        if not self.sign_model:
            return {
                'predictions': [],
                'message': 'Sign model not loaded',
                'has_model': False
            }
        
        try:
            # ì‹œí€€ìŠ¤ ê¸¸ì´ í™•ì¸
            if len(feature_sequence) < self.sequence_length:
                return {
                    'predictions': [],
                    'message': f'Need at least {self.sequence_length} frames, got {len(feature_sequence)}',
                    'has_model': True
                }
            
            # ì‹œí€€ìŠ¤ë¥¼ ì •í™•í•œ ê¸¸ì´ë¡œ ìë¥´ê¸°/íŒ¨ë”©
            if len(feature_sequence) > self.sequence_length:
                feature_sequence = feature_sequence[-self.sequence_length:]
            elif len(feature_sequence) < self.sequence_length:
                # íŒ¨ë”© (ì•ë¶€ë¶„ì„ 0ìœ¼ë¡œ ì±„ì›€)
                padding_length = self.sequence_length - len(feature_sequence)
                padding = [np.zeros_like(feature_sequence[0]) for _ in range(padding_length)]
                feature_sequence = padding + feature_sequence
            
            # ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„± (ì‹¤ì œ ë°ì´í„°ê°€ ìˆëŠ” ë¶€ë¶„ë§Œ True)
            attention_mask = torch.zeros(self.sequence_length, dtype=torch.bool)
            actual_length = len([f for f in feature_sequence if not np.all(f == 0)])
            if actual_length > 0:
                attention_mask[-actual_length:] = True
            
            # í…ì„œë¡œ ë³€í™˜
            features = np.array(feature_sequence)
            features = torch.FloatTensor(features).unsqueeze(0).to(self.device)  # (1, seq_len, feature_dim)
            attention_mask = attention_mask.unsqueeze(0).to(self.device)  # (1, seq_len)
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            with torch.no_grad():
                outputs = self.sign_model(features, attention_mask=attention_mask)
                probabilities = torch.softmax(outputs, dim=1)
                confidences, predicted_indices = torch.topk(probabilities, k=3)  # top 3 predictions
            
            # ê²°ê³¼ ì •ë¦¬
            predictions = []
            for conf, idx in zip(confidences[0], predicted_indices[0]):
                conf_val = float(conf.cpu())
                idx_val = int(idx.cpu())
                
                if idx_val < len(self.class_names):
                    class_name = self.class_names[idx_val]
                    if class_name not in {"<PAD>", "<UNK>", "< SOS >", "<EOS>"} and conf_val >= confidence_threshold:
                        predictions.append({
                            'class': class_name,  # 'word'ë¥¼ 'class'ë¡œ ë³€ê²½
                            'confidence': conf_val
                        })
        
            if len(predictions) > 0:
                print("\nğŸ¤Ÿ ìˆ˜ì–´ ì¸ì‹ ê²°ê³¼:")
                print("-" * 40)
                for pred in predictions:
                    print(f"ë‹¨ì–´: {pred['class']:<15} - ì‹ ë¢°ë„: {pred['confidence']*100:>6.2f}%")
                print("-" * 40)
            
            return {
                'predictions': predictions,
                'message': 'Transformer prediction completed',
                'has_model': True,
                'model_type': 'OpenHands Transformer'
            }
            
        except Exception as e:
            print(f"âŒ ìˆ˜ì–´ ì¸ì‹ ì‹¤íŒ¨: {e}")
            return {
                'predictions': [],
                'error': str(e),
                'has_model': True,
                'model_type': 'OpenHands Transformer'
            }

class PoseServer:
    """í¬ì¦ˆ ì¶”ì • ì„œë²„ (ìˆ˜ì–´ ì¸ì‹ ê¸°ëŠ¥ ì¶”ê°€)"""
    
    def __init__(self, 
                 rtmw_config: str,
                 rtmw_checkpoint: str,
                 device: str = "auto",
                 port: int = 5000,
                 host: str = "0.0.0.0",
                 detection_conf: float = 0.5,
                 yolo_model_size: str = "n",
                 sign_model_path: str = None):
        
        self.port = port
        self.host = host
        self.detection_conf = detection_conf
        self.yolo_model_size = yolo_model_size
        
        # YOLOv11 ê²€ì¶œê¸° ì´ˆê¸°í™” (1ëª…ë§Œ ê²€ì¶œ)
        self.detector = PersonDetector(device, yolo_model_size)
        
        # RTMW ì¶”ì •ê¸° ì´ˆê¸°í™”
        self.estimator = RTMWPoseEstimator(rtmw_config, rtmw_checkpoint, device)
        
        # ìˆ˜ì–´ ì¸ì‹ê¸° ì´ˆê¸°í™”
        self.sign_recognizer = SignLanguageRecognizer(sign_model_path)
        
        # Flask ì•± ì„¤ì •
        self.app = Flask(__name__)
        self.setup_routes()
        
        # ì„±ëŠ¥ í†µê³„
        self.request_count = 0
        self.processing_times = deque(maxlen=100)
        self.detection_times = deque(maxlen=100)
        self.sign_request_count = 0
        self.sign_processing_times = deque(maxlen=100)
        
        # ìˆ˜ì–´ ì¸ì‹ìš© íŠ¹ì§• ë²„í¼ (í´ë¼ì´ì–¸íŠ¸ë³„ë¡œ ê´€ë¦¬)
        self.feature_buffers = {}  # {client_id: deque}
        
        print(f"âœ… í¬ì¦ˆ ì„œë²„ ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   - YOLO ëª¨ë¸: v11{yolo_model_size} (ì„±ëŠ¥: v8 ëŒ€ë¹„ ë¹ ë¥´ê³  ì •í™•)")
        print(f"   - ê²€ì¶œ ë””ë°”ì´ìŠ¤: {self.detector.device}")
        print(f"   - í¬ì¦ˆ ë””ë°”ì´ìŠ¤: {self.estimator.device}")
        print(f"   - ê²€ì¶œ ì‹ ë¢°ë„: {detection_conf}")
        print(f"   - ì„œë²„ ì£¼ì†Œ: {host}:{port}")
        print(f"   - ê²€ì¶œ ëª¨ë“œ: 1ëª…ë§Œ ê²€ì¶œ (ìµœê³  ì‹ ë¢°ë„)")
        print(f"   - ìˆ˜ì–´ ì¸ì‹: {'OpenHands Transformer ëª¨ë¸ í™œì„±í™”ë¨' if self.sign_recognizer.sign_model else 'íŠ¹ì§•ì¶”ì¶œë§Œ ê°€ëŠ¥'}")

    def setup_routes(self):
        """Flask ë¼ìš°íŠ¸ ì„¤ì •"""
        
        @self.app.route('/health', methods=['GET'])
        def health_check():
            """í—¬ìŠ¤ ì²´í¬"""
            return jsonify({
                'status': 'healthy',
                'detection_device': self.detector.device,
                'pose_device': self.estimator.device,
                'request_count': self.request_count,
                'sign_request_count': self.sign_request_count,
                'avg_processing_time': np.mean(self.processing_times) if self.processing_times else 0,
                'avg_detection_time': np.mean(self.detection_times) if self.detection_times else 0,
                'avg_sign_processing_time': np.mean(self.sign_processing_times) if self.sign_processing_times else 0,
                'detection_mode': 'single_person_best_confidence',
                'features': ['pose_estimation', 'sign_recognition'],
                'sign_model_loaded': self.sign_recognizer.sign_model is not None,
                'sign_model_type': 'OpenHands Transformer' if self.sign_recognizer.sign_model else None
            })
        
        @self.app.route('/estimate_pose', methods=['POST'])
        def estimate_pose():
            """ê¸°ì¡´ í¬ì¦ˆ ì¶”ì • ì—”ë“œí¬ì¸íŠ¸ (ë³€ê²½ ì—†ìŒ)"""
            try:
                start_time = time.time()
                
                # ìš”ì²­ ë°ì´í„° ê²€ì¦
                if 'image' not in request.files:
                    return jsonify({'error': 'No image provided'}), 400
                
                image_file = request.files['image']
                if image_file.filename == '':
                    return jsonify({'error': 'Empty image file'}), 400
                
                # ë©”íƒ€ë°ì´í„° íŒŒì‹±
                frame_id = request.form.get('frame_id', 0)
                timestamp = float(request.form.get('timestamp', time.time()))
                detection_only = request.form.get('detection_only', 'false').lower() == 'true'
                
                # ì´ë¯¸ì§€ ë””ì½”ë”©
                image_data = image_file.read()
                nparr = np.frombuffer(image_data, np.uint8)
                original_image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
                
                if original_image is None:
                    return jsonify({'error': 'Failed to decode image'}), 400
                
                orig_h, orig_w = original_image.shape[:2]
                
                # 1. ì‚¬ëŒ ê²€ì¶œ (1ëª…ë§Œ, ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„)
                detection_start = time.time()
                best_person = self.detector.detect_best_person(original_image, self.detection_conf)
                detection_time = time.time() - detection_start
                self.detection_times.append(detection_time)
                
                if best_person is None:
                    return jsonify({
                        'frame_id': frame_id,
                        'error': 'No person detected',
                        'detection_time': detection_time,
                        'total_time': time.time() - start_time,
                        'timestamp': timestamp,
                        'received_at': start_time,
                        'original_size': [orig_w, orig_h],
                        'detected_persons': 0,
                        'detection_mode': 'single_person_best_confidence'
                    })
                
                # 2. í¬ë¡­ ë° ë¦¬ì‚¬ì´ì¦ˆ (288x384)
                crop_image = self.estimator.crop_and_resize_person(original_image, best_person)
                
                # 3. í¬ì¦ˆ ì¶”ì •
                keypoints, scores, pose_time = self.estimator.estimate_pose_on_crop(crop_image)
                
                # 4. í‚¤í¬ì¸íŠ¸ ì¢Œí‘œ ì²˜ë¦¬
                # í¬ë¡­ëœ ì˜ì—­ì˜ ì •ë³´
                x1, y1, x2, y2 = best_person[:4]
                crop_w = x2 - x1
                crop_h = y2 - y1
                
                # 288x384 ê¸°ì¤€ í‚¤í¬ì¸íŠ¸ (ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸ìš©) - ì›ë³¸ ê·¸ëŒ€ë¡œ ìœ ì§€
                keypoints_288x384 = keypoints.copy()
                
                # ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜ëœ í‚¤í¬ì¸íŠ¸ (ìŠ¤ì¼ˆë ˆí†¤ ì‹œê°í™”ìš©)
                keypoints_original = keypoints.copy()
                if keypoints_original.size > 0:
                    # 288x384ì—ì„œ í¬ë¡­ ì˜ì—­ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§
                    keypoints_original[:, 0] = keypoints_original[:, 0] * (crop_w / 288.0) + x1
                    keypoints_original[:, 1] = keypoints_original[:, 1] * (crop_h / 384.0) + y1
                
                # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
                total_time = time.time() - start_time
                self.processing_times.append(total_time)
                self.request_count += 1
                
                response = {
                    'frame_id': frame_id,
                    'keypoints': keypoints_original.tolist(),  # ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œ (ìŠ¤ì¼ˆë ˆí†¤ ì‹œê°í™”ìš©)
                    'keypoints_288x384': keypoints_288x384.tolist(),  # 288x384 ì¢Œí‘œ (ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸ìš©)
                    'scores': scores.tolist(),
                    'person_box': best_person,
                    'detection_time': detection_time,
                    'pose_time': pose_time,
                    'total_time': total_time,
                    'timestamp': timestamp,
                    'received_at': start_time,
                    'yolo_model': f'v11{self.yolo_model_size}',
                    'detection_device': self.detector.device,
                    'pose_device': self.estimator.device,
                    'original_size': [orig_w, orig_h],
                    'crop_size': [288, 384],
                    'detected_persons': 1,
                    'detection_mode': 'single_person_best_confidence'
                }
                
                # ì£¼ê¸°ì  ë¡œê·¸ ì¶œë ¥
                if self.request_count % 30 == 0:
                    avg_time = np.mean(self.processing_times) if self.processing_times else 0
                    avg_det_time = np.mean(self.detection_times) if self.detection_times else 0
                    logger.info(f"ğŸ“Š ìš”ì²­ {self.request_count}: í‰ê·  ì²˜ë¦¬ì‹œê°„ {avg_time*1000:.1f}ms (ê²€ì¶œ: {avg_det_time*1000:.1f}ms)")
                
                return jsonify(response)
                
            except Exception as e:
                logger.error(f"âŒ í¬ì¦ˆ ì¶”ì • ìš”ì²­ ì‹¤íŒ¨: {e}")
                return jsonify({'error': str(e)}), 500
        
        @self.app.route('/extract_sign_features', methods=['POST'])
        def extract_sign_features():
            """ìˆ˜ì–´ íŠ¹ì§• ì¶”ì¶œ ì—”ë“œí¬ì¸íŠ¸ (MediaPipe ê¸°ë°˜)"""
            try:
                start_time = time.time()
                
                # ìš”ì²­ ë°ì´í„° ê²€ì¦
                if 'image' not in request.files:
                    return jsonify({'error': 'No image provided'}), 400
                
                image_file = request.files['image']
                if image_file.filename == '':
                    return jsonify({'error': 'Empty image file'}), 400
                
                # ë©”íƒ€ë°ì´í„° íŒŒì‹±
                frame_id = request.form.get('frame_id', 0)
                timestamp = float(request.form.get('timestamp', time.time()))
                client_id = request.form.get('client_id', 'default')
                
                # ì´ë¯¸ì§€ ë””ì½”ë”©
                image_data = image_file.read()
                nparr = np.frombuffer(image_data, np.uint8)
                original_image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
                
                if original_image is None:
                    return jsonify({'error': 'Failed to decode image'}), 400
                
                orig_h, orig_w = original_image.shape[:2]
                
                # MediaPipeë¡œ íŠ¹ì§• ì¶”ì¶œ
                extraction_start = time.time()
                features = self.sign_recognizer.extract_features_from_image(original_image)
                extraction_time = time.time() - extraction_start
                
                if features is None:
                    return jsonify({
                        'frame_id': frame_id,
                        'error': 'No hand or pose landmarks detected',
                        'extraction_time': extraction_time,
                        'total_time': time.time() - start_time,
                        'timestamp': timestamp,
                        'received_at': start_time,
                        'original_size': [orig_w, orig_h],
                        'method': 'mediapipe'
                    }), 200
                
                # í´ë¼ì´ì–¸íŠ¸ë³„ íŠ¹ì§• ë²„í¼ ê´€ë¦¬
                if client_id not in self.feature_buffers:
                    self.feature_buffers[client_id] = deque(maxlen=self.sign_recognizer.sequence_length)
                
                self.feature_buffers[client_id].append(features)
                
                # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
                total_time = time.time() - start_time
                self.sign_processing_times.append(total_time)
                self.sign_request_count += 1
                
                response = {
                    'frame_id': frame_id,
                    'features': features.tolist(),
                    'features_shape': features.shape,
                    'buffer_length': len(self.feature_buffers[client_id]),
                    'buffer_max_length': self.sign_recognizer.sequence_length,
                    'extraction_time': extraction_time,
                    'total_time': total_time,
                    'timestamp': timestamp,
                    'received_at': start_time,
                    'original_size': [orig_w, orig_h],
                    'method': 'mediapipe',
                    'client_id': client_id
                }
                
                return jsonify(response)
                
            except Exception as e:
                logger.error(f"âŒ ìˆ˜ì–´ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                return jsonify({'error': str(e)}), 500
        
        @self.app.route('/predict_sign', methods=['POST'])
        def predict_sign():
            """ìˆ˜ì–´ ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸"""
            try:
                start_time = time.time()
                
                # ë©”íƒ€ë°ì´í„° íŒŒì‹±
                frame_id = request.form.get('frame_id', 0)
                timestamp = float(request.form.get('timestamp', time.time()))
                client_id = request.form.get('client_id', 'default')
                confidence_threshold = float(request.form.get('confidence_threshold', 0.6))
                
                # í´ë¼ì´ì–¸íŠ¸ ë²„í¼ í™•ì¸
                if client_id not in self.feature_buffers:
                    return jsonify({
                        'frame_id': frame_id,
                        'error': 'No feature buffer found for client. Call extract_sign_features first.',
                        'client_id': client_id,
                        'timestamp': timestamp,
                        'total_time': time.time() - start_time
                    }), 400
                
                feature_buffer = self.feature_buffers[client_id]
                
                if len(feature_buffer) == 0:
                    return jsonify({
                        'frame_id': frame_id,
                        'error': 'Feature buffer is empty',
                        'client_id': client_id,
                        'timestamp': timestamp,
                        'total_time': time.time() - start_time
                    }), 400
                
                # ìˆ˜ì–´ ì˜ˆì¸¡ ìˆ˜í–‰
                prediction_start = time.time()
                feature_sequence = list(feature_buffer)
                sign_results = self.sign_recognizer.predict_sign_sequence(
                    feature_sequence, confidence_threshold
                )
                prediction_time = time.time() - prediction_start
                
                # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
                total_time = time.time() - start_time
                
                response = {
                    'frame_id': frame_id,
                    'client_id': client_id,
                    'sign_predictions': sign_results.get('predictions', []),
                    'buffer_length': len(feature_buffer),
                    'sequence_length_required': self.sign_recognizer.sequence_length,
                    'prediction_time': prediction_time,
                    'total_time': total_time,
                    'timestamp': timestamp,
                    'received_at': start_time,
                    'confidence_threshold': confidence_threshold,
                    'message': sign_results.get('message', ''),
                    'error': sign_results.get('error', None),
                    'has_model': sign_results.get('has_model', False),
                    'model_type': sign_results.get('model_type', 'OpenHands Transformer')
                }
                
                # ì£¼ê¸°ì  ë¡œê·¸ ì¶œë ¥
                if len(sign_results.get('predictions', [])) > 0:
                    best_pred = sign_results['predictions'][0]
                    logger.info(f"ğŸ¤Ÿ ìˆ˜ì–´ ì˜ˆì¸¡: {best_pred['word']} (ì‹ ë¢°ë„: {best_pred['confidence']:.2f})")
                
                return jsonify(response)
                
            except Exception as e:
                logger.error(f"âŒ ìˆ˜ì–´ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                return jsonify({'error': str(e)}), 500
        
        @self.app.route('/sign_recognition', methods=['POST'])
        def sign_recognition():
            """í†µí•© ìˆ˜ì–´ ì¸ì‹ ì—”ë“œí¬ì¸íŠ¸ (íŠ¹ì§• ì¶”ì¶œ + ì˜ˆì¸¡)"""
            try:
                start_time = time.time()
                
                # ìš”ì²­ ë°ì´í„° ê²€ì¦
                if 'image' not in request.files:
                    return jsonify({'error': 'No image provided'}), 400
                
                image_file = request.files['image']
                if image_file.filename == '':
                    return jsonify({'error': 'Empty image file'}), 400
                
                # ë©”íƒ€ë°ì´í„° íŒŒì‹±
                frame_id = request.form.get('frame_id', 0)
                timestamp = float(request.form.get('timestamp', time.time()))
                client_id = request.form.get('client_id', 'default')
                confidence_threshold = float(request.form.get('confidence_threshold', 0.6))
                extract_only = request.form.get('extract_only', 'false').lower() == 'true'
                
                # ì´ë¯¸ì§€ ë””ì½”ë”©
                image_data = image_file.read()
                nparr = np.frombuffer(image_data, np.uint8)
                original_image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
                
                if original_image is None:
                    return jsonify({'error': 'Failed to decode image'}), 400
                
                orig_h, orig_w = original_image.shape[:2]
                
                # 1. íŠ¹ì§• ì¶”ì¶œ
                extraction_start = time.time()
                features = self.sign_recognizer.extract_features_from_image(original_image)
                extraction_time = time.time() - extraction_start
                
                if features is None:
                    return jsonify({
                        'frame_id': frame_id,
                        'error': 'No hand or pose landmarks detected',
                        'extraction_time': extraction_time,
                        'total_time': time.time() - start_time,
                        'timestamp': timestamp,
                        'received_at': start_time,
                        'original_size': [orig_w, orig_h],
                        'method': 'integrated',
                        'client_id': client_id
                    }), 200
                
                # í´ë¼ì´ì–¸íŠ¸ë³„ íŠ¹ì§• ë²„í¼ ê´€ë¦¬
                if client_id not in self.feature_buffers:
                    self.feature_buffers[client_id] = deque(maxlen=self.sign_recognizer.sequence_length)
                
                self.feature_buffers[client_id].append(features)
                
                # íŠ¹ì§• ì¶”ì¶œë§Œ ìš”ì²­í•œ ê²½ìš°
                if extract_only:
                    response = {
                        'frame_id': frame_id,
                        'client_id': client_id,
                        'features': features.tolist(),
                        'buffer_length': len(self.feature_buffers[client_id]),
                        'extraction_time': extraction_time,
                        'total_time': time.time() - start_time,
                        'timestamp': timestamp,
                        'received_at': start_time,
                        'original_size': [orig_w, orig_h],
                        'method': 'integrated'
                    }
                    return jsonify(response)
                
                # 2. ìˆ˜ì–´ ì˜ˆì¸¡ ìˆ˜í–‰
                prediction_start = time.time()
                feature_sequence = list(self.feature_buffers[client_id])
                sign_results = self.sign_recognizer.predict_sign_sequence(
                    feature_sequence, confidence_threshold
                )
                prediction_time = time.time() - prediction_start
                
                # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
                total_time = time.time() - start_time
                self.sign_processing_times.append(total_time)
                self.sign_request_count += 1
                
                response = {
                    'frame_id': frame_id,
                    'client_id': client_id,
                    'features': features.tolist(),
                    'sign_predictions': sign_results.get('predictions', []),
                    'buffer_length': len(self.feature_buffers[client_id]),
                    'sequence_length_required': self.sign_recognizer.sequence_length,
                    'extraction_time': extraction_time,
                    'prediction_time': prediction_time,
                    'total_time': total_time,
                    'timestamp': timestamp,
                    'received_at': start_time,
                    'original_size': [orig_w, orig_h],
                    'confidence_threshold': confidence_threshold,
                    'method': 'integrated',
                    'message': sign_results.get('message', ''),
                    'error': sign_results.get('error', None),
                    'has_model': sign_results.get('has_model', False),
                    'model_type': sign_results.get('model_type', 'OpenHands Transformer')
                }
                
                # ì£¼ê¸°ì  ë¡œê·¸ ì¶œë ¥
                if self.sign_request_count % 10 == 0:
                    avg_time = np.mean(self.sign_processing_times) if self.sign_processing_times else 0
                    logger.info(f"ğŸ¤Ÿ ìˆ˜ì–´ ì¸ì‹ ìš”ì²­ {self.sign_request_count}: í‰ê·  ì²˜ë¦¬ì‹œê°„ {avg_time*1000:.1f}ms")
                
                if len(sign_results.get('predictions', [])) > 0:
                    best_pred = sign_results['predictions'][0]
                    logger.info(f"ğŸ¯ ì¸ì‹ëœ ìˆ˜ì–´: {best_pred['class']} (ì‹ ë¢°ë„: {best_pred['confidence']:.2f})")
                
                print(f"\nğŸ¤Ÿ í†µí•© ìˆ˜ì–´ ì¸ì‹ ê²°ê³¼: {jsonify(response)}")
                
                return jsonify(response)
                
            except Exception as e:
                logger.error(f"âŒ ìˆ˜ì–´ ì¸ì‹ ì‹¤íŒ¨: {e}")
                return jsonify({'error': str(e)}), 500
        
        @self.app.route('/clear_buffer/<client_id>', methods=['POST'])
        def clear_buffer(client_id):
            """í´ë¼ì´ì–¸íŠ¸ë³„ íŠ¹ì§• ë²„í¼ í´ë¦¬ì–´"""
            if client_id in self.feature_buffers:
                self.feature_buffers[client_id].clear()
                return jsonify({
                    'message': f'Buffer cleared for client {client_id}',
                    'client_id': client_id
                })
            else:
                return jsonify({
                    'message': f'No buffer found for client {client_id}',
                    'client_id': client_id
                }), 404
        
        @self.app.route('/stats', methods=['GET'])
        def get_stats():
            """í†µê³„ ì •ë³´"""
            buffer_stats = {}
            for client_id, buffer in self.feature_buffers.items():
                buffer_stats[client_id] = len(buffer)
            
            return jsonify({
                'request_count': self.request_count,
                'sign_request_count': self.sign_request_count,
                'detection_device': self.detector.device,
                'pose_device': self.estimator.device,
                'detection_conf': self.detection_conf,
                'detection_mode': 'single_person_best_confidence',
                'sign_model_loaded': self.sign_recognizer.sign_model is not None,
                'sign_model_type': 'OpenHands Transformer' if self.sign_recognizer.sign_model else None,
                'sign_sequence_length': self.sign_recognizer.sequence_length,
                'active_clients': len(self.feature_buffers),
                'client_buffer_stats': buffer_stats,
                'processing_times': {
                    'count': len(self.processing_times),
                    'mean': float(np.mean(self.processing_times)) if self.processing_times else 0,
                    'min': float(np.min(self.processing_times)) if self.processing_times else 0,
                    'max': float(np.max(self.processing_times)) if self.processing_times else 0,
                    'std': float(np.std(self.processing_times)) if self.processing_times else 0
                },
                'detection_times': {
                    'count': len(self.detection_times),
                    'mean': float(np.mean(self.detection_times)) if self.detection_times else 0,
                    'min': float(np.min(self.detection_times)) if self.detection_times else 0,
                    'max': float(np.max(self.detection_times)) if self.detection_times else 0,
                    'std': float(np.std(self.detection_times)) if self.detection_times else 0
                },
                'sign_processing_times': {
                    'count': len(self.sign_processing_times),
                    'mean': float(np.mean(self.sign_processing_times)) if self.sign_processing_times else 0,
                    'min': float(np.min(self.sign_processing_times)) if self.sign_processing_times else 0,
                    'max': float(np.max(self.sign_processing_times)) if self.sign_processing_times else 0,
                    'std': float(np.std(self.sign_processing_times)) if self.sign_processing_times else 0
                },
                'features': ['pose_estimation', 'sign_recognition', 'feature_extraction']
            })
    
    def run(self):
        """ì„œë²„ ì‹¤í–‰"""
        print(f"\nğŸš€ í¬ì¦ˆ ì¶”ì • ì„œë²„ ì‹œì‘")
        print(f"   - ì£¼ì†Œ: http://{self.host}:{self.port}")
        print(f"   - í—¬ìŠ¤ì²´í¬: http://{self.host}:{self.port}/health")
        print(f"   - í†µê³„: http://{self.host}:{self.port}/stats")
        print(f"   - í¬ì¦ˆ ì¶”ì •: POST http://{self.host}:{self.port}/estimate_pose")
        print(f"   - ìˆ˜ì–´ íŠ¹ì§• ì¶”ì¶œ: POST http://{self.host}:{self.port}/extract_sign_features")
        print(f"   - ìˆ˜ì–´ ì˜ˆì¸¡: POST http://{self.host}:{self.port}/predict_sign")
        print(f"   - í†µí•© ìˆ˜ì–´ ì¸ì‹: POST http://{self.host}:{self.port}/sign_recognition")
        print(f"   - ë²„í¼ í´ë¦¬ì–´: POST http://{self.host}:{self.port}/clear_buffer/<client_id>")
        print(f"   - ê²€ì¶œ ëª¨ë“œ: 1ëª…ë§Œ ê²€ì¶œ (ìµœê³  ì‹ ë¢°ë„)")
        print(f"   - ìˆ˜ì–´ ëª¨ë¸: {'OpenHands Transformer' if self.sign_recognizer.sign_model else 'íŠ¹ì§•ì¶”ì¶œë§Œ'}")
        print(f"   - Ctrl+Cë¡œ ì¢…ë£Œ")
        
        try:
            # Flask ì•± ì‹¤í–‰ (ë””ë²„ê·¸ ëª¨ë“œ ë¹„í™œì„±í™”, í”„ë¡œë•ì…˜ìš©)
            self.app.run(
                host=self.host,
                port=self.port,
                debug=False,
                threaded=True,  # ë©€í‹°ìŠ¤ë ˆë“œ ì§€ì›
                use_reloader=False
            )
        except KeyboardInterrupt:
            print("\nâ¹ï¸ í¬ì¦ˆ ì„œë²„ ì¢…ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ì„œë²„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")

def main():
    parser = argparse.ArgumentParser(description="Enhanced RTMW Pose Estimation Server with OpenHands Transformer Sign Language Recognition")
    parser.add_argument("--config", type=str, 
                       default="configs/wholebody_2d_keypoint/rtmpose/cocktail14/rtmw-l_8xb320-270e_cocktail14-384x288.py",
                       help="RTMW ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--checkpoint", type=str,
                       default="models/rtmw-dw-x-l_simcc-cocktail14_270e-384x288-20231122.pth",
                       help="RTMW ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ") 
    parser.add_argument("--device", type=str, default="auto",
                       choices=["auto", "cpu", "cuda", "xpu"],
                       help="ì¶”ë¡  ë””ë°”ì´ìŠ¤ (ê¸°ë³¸ê°’: auto - XPU > CUDA > CPU ìˆœì„œ)")
    parser.add_argument("--yolo-model", type=str, default="n",
                       choices=["n", "s", "m", "l", "x"],
                       help="YOLO ëª¨ë¸ í¬ê¸° (ê¸°ë³¸ê°’: n=nano, s=small, m=medium, l=large, x=xlarge)")
    parser.add_argument("--port", type=int, default=5000,
                       help="ì„œë²„ í¬íŠ¸ (ê¸°ë³¸ê°’: 5000)")
    parser.add_argument("--host", type=str, default="0.0.0.0",
                       help="ì„œë²„ í˜¸ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: 0.0.0.0)")
    parser.add_argument("--detection-conf", type=float, default=0.5,
                       help="ì‚¬ëŒ ê²€ì¶œ ì‹ ë¢°ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.5)")
    parser.add_argument("--sign-model", type=str, default=None,
                       help="OpenHands fine-tuned Transformer ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸ ê²½ë¡œ (ì˜ˆ: best_model.pt)")
    
    args = parser.parse_args()
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not Path(args.config).exists():
        print(f"âŒ ì„¤ì • íŒŒì¼ ì—†ìŒ: {args.config}")
        return
    
    if not Path(args.checkpoint).exists():
        print(f"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {args.checkpoint}")
        return
    
    # ìˆ˜ì–´ ëª¨ë¸ íŒŒì¼ í™•ì¸ (ì˜µì…˜)
    if args.sign_model and not Path(args.sign_model).exists():
        print(f"âš ï¸ ìˆ˜ì–´ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {args.sign_model} (íŠ¹ì§• ì¶”ì¶œë§Œ ê°€ëŠ¥)")
        args.sign_model = None
    
    # YOLO ëª¨ë¸ ì„±ëŠ¥ ì •ë³´ ì¶œë ¥
    model_info = {
        "n": "nano - ê°€ì¥ ë¹ ë¦„, ê°€ë²¼ì›€ (3.3M íŒŒë¼ë¯¸í„°)",
        "s": "small - ì†ë„ì™€ ì •í™•ë„ ê· í˜• (11.2M íŒŒë¼ë¯¸í„°)", 
        "m": "medium - ë” ë†’ì€ ì •í™•ë„ (20.1M íŒŒë¼ë¯¸í„°)",
        "l": "large - ë†’ì€ ì •í™•ë„ (25.3M íŒŒë¼ë¯¸í„°)",
        "x": "xlarge - ìµœê³  ì •í™•ë„, ëŠë¦¼ (68.2M íŒŒë¼ë¯¸í„°)"
    }
    
    print(f"\nğŸš€ Enhanced Pose Server ì •ë³´:")
    print(f"   - YOLOv11{args.yolo_model} ({model_info[args.yolo_model]})")
    print(f"   - RTMW í¬ì¦ˆ ì¶”ì •: í™œì„±í™”")
    print(f"   - ìˆ˜ì–´ ì¸ì‹: {'OpenHands Transformer ëª¨ë¸ ë¡œë“œë¨' if args.sign_model else 'MediaPipe íŠ¹ì§•ì¶”ì¶œë§Œ'}")
    print(f"   - YOLOv8 ëŒ€ë¹„: 22% ì ì€ íŒŒë¼ë¯¸í„°ë¡œ ë” ë†’ì€ mAP")
    print(f"   - ì¶”ë¡  ì†ë„: YOLOv8 ëŒ€ë¹„ í‰ê·  40% í–¥ìƒ")
    
    if args.device == "auto":
        print(f"\nğŸ”§ ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ ìˆœì„œ:")
        print(f"   1. XPU (Intel Arc GPU) - ê°€ì¥ ê¶Œì¥")
        print(f"   2. CUDA (NVIDIA GPU)")  
        print(f"   3. CPU (í´ë°±)")
    
    # Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
    if args.sign_model and not TRANSFORMERS_AVAILABLE:
        print(f"\nâš ï¸ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”:")
        print(f"   pip install transformers")
        print(f"   í˜„ì¬ëŠ” íŠ¹ì§• ì¶”ì¶œë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        args.sign_model = None
    
    try:
        pose_server = PoseServer(
            rtmw_config=args.config,
            rtmw_checkpoint=args.checkpoint,
            device=args.device,
            port=args.port,
            host=args.host,
            detection_conf=args.detection_conf,
            yolo_model_size=args.yolo_model,
            sign_model_path=args.sign_model
        )
        
        pose_server.run()
        
    except Exception as e:
        print(f"âŒ í¬ì¦ˆ ì„œë²„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    main()